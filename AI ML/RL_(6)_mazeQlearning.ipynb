{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Maze with Q-learning  ( Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maze generator consists of maze class and field class. It generates a square shaped maze. The maze has route tiles, wall and block tiles, starting and goal point. The route tiles have -1 or 0 on it, which is the point you can get by stepping it. Apparently you will get 1 point subtracted if you step on -1 tile. The wall and block tiles, in #, are where you cannot interude. You have to bypass #. The starting point, namely S, is where you start the maze and goal point, which is shown as 50, is where you head to. You will earn 50 points when you made to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of this notebook is to solve self-made maze with Q-learning.\n",
    "### The maze is in square shape, consists of start point, goal point and tiles in the mid of them.\n",
    "### Each tile has numericals as its point. In other words, if you step on to the tile with -1, you get 1 point subtracted.\n",
    "### The maze has blocks to prevent you from taking the route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pds\n",
    "import random\n",
    "import copy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from collections import deque\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    def __init__(self, size=10, blocks_rate=0.1):\n",
    "        self.size = size if size > 3 else 10\n",
    "        self.blocks = int((size ** 2) * blocks_rate) \n",
    "        self.s_list = []\n",
    "        self.maze_list = []\n",
    "        self.e_list = []\n",
    "\n",
    "    def create_mid_lines(self, k):\n",
    "        if k == 0: self.maze_list.append(self.s_list)\n",
    "        elif k == self.size - 1: self.maze_list.append(self.e_list)\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            for l in range(0,self.size):\n",
    "                if l == 0: tmp_list.extend(\"#\")\n",
    "                elif l == self.size-1: tmp_list.extend(\"#\")\n",
    "                else:\n",
    "                    a = random.randint(-1, 0)\n",
    "                    tmp_list.extend([a])\n",
    "            self.maze_list.append(tmp_list)\n",
    "\n",
    "    def insert_blocks(self, k, s_r, e_r):\n",
    "        b_y = random.randint(1, self.size-2)\n",
    "        b_x = random.randint(1, self.size-2)\n",
    "        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1\n",
    "        else: self.maze_list[b_y][b_x] = \"#\"\n",
    "            \n",
    "    def generate_maze(self): \n",
    "        s_r = random.randint(1, (self.size / 2) - 1)\n",
    "        for i in range(0, self.size):\n",
    "            if i == s_r: self.s_list.extend(\"S\")\n",
    "            else: self.s_list.extend(\"#\")\n",
    "        start_point = [0, s_r]\n",
    "\n",
    "        e_r = random.randint((self.size / 2) + 1, self.size - 2)\n",
    "        for j in range(0, self.size):\n",
    "            if j == e_r: self.e_list.extend([50])\n",
    "            else: self.e_list.extend(\"#\")\n",
    "        goal_point = [self.size - 1, e_r]\n",
    "\n",
    "        for k in range(0, self.size):\n",
    "            self.create_mid_lines(k)\n",
    "        \n",
    "        for k in range(self.blocks):\n",
    "            self.insert_blocks(k, s_r, e_r)\n",
    "\n",
    "        return self.maze_list, start_point, goal_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field(object):\n",
    "    def __init__(self, maze, start_point, goal_point):\n",
    "        self.maze = maze\n",
    "        self.start_point = start_point\n",
    "        self.goal_point = goal_point\n",
    "        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    def display(self, point=None):\n",
    "        field_data = copy.deepcopy(self.maze)\n",
    "        if not point is None:\n",
    "                y, x = point\n",
    "                field_data[y][x] = \"@@\"\n",
    "        else:\n",
    "                point = \"\"\n",
    "        for line in field_data:\n",
    "                print (\"\\t\" + \"%3s \" * len(line) % tuple(line))\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        movables = []\n",
    "        if state == self.start_point:\n",
    "            y = state[0] + 1\n",
    "            x = state[1]\n",
    "            a = [[y, x]]\n",
    "            return a\n",
    "        else:\n",
    "            for v in self.movable_vec:\n",
    "                y = state[0] + v[0]\n",
    "                x = state[1] + v[1]\n",
    "                if not(0 < x < len(self.maze) and\n",
    "                       0 <= y <= len(self.maze) - 1 and\n",
    "                       maze[y][x] != \"#\" and\n",
    "                       maze[y][x] != \"S\"):\n",
    "                    continue\n",
    "                movables.append([y,x])\n",
    "            if len(movables) != 0:\n",
    "                return movables\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def get_val(self, state):\n",
    "        y, x = state\n",
    "        if state == self.start_point: return 0, False\n",
    "        else:\n",
    "            v = float(self.maze[y][x])\n",
    "            if state == self.goal_point: \n",
    "                return v, True\n",
    "            else: \n",
    "                return v, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "barriar_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_1 = Maze(size, barriar_rate)\n",
    "maze, start_point, goal_point = maze_1.generate_maze()\n",
    "maze_field = Field(maze, start_point, goal_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n"
     ]
    }
   ],
   "source": [
    "maze_field.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Solving the maze in Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning_Solver(object):\n",
    "    def __init__(self, maze, display=False):\n",
    "        self.Qvalue = {}\n",
    "        self.Field = maze\n",
    "        self.alpha = 0.2\n",
    "        self.gamma  = 0.9\n",
    "        self.epsilon = 0.2\n",
    "        self.steps = 0\n",
    "        self.score = 0\n",
    "        self.display = display\n",
    "\n",
    "    def qlearn(self, greedy_flg=False):\n",
    "        state = self.Field.start_point\n",
    "        while True:\n",
    "            if greedy_flg:\n",
    "                self.steps += 1\n",
    "                action = self.choose_action_greedy(state)\n",
    "                print(\"current state: {0} -> action: {1} \".format(state, action))\n",
    "                if self.display:\n",
    "                    self.Field.display(action)\n",
    "                reward, tf = self.Field.get_val(action)\n",
    "                self.score =  self.score + reward\n",
    "                print(\"current step: {0} \\t score: {1}\\n\".format(self.steps, self.score))\n",
    "                if tf == True:\n",
    "                    print(\"Goal!\")\n",
    "                    break\n",
    "            else:\n",
    "                action = self.choose_action(state)    \n",
    "            if self.update_Qvalue(state, action):\n",
    "                break\n",
    "            else:\n",
    "                state = action\n",
    "\n",
    "    def update_Qvalue(self, state, action):\n",
    "        Q_s_a = self.get_Qvalue(state, action)\n",
    "        mQ_s_a = max([self.get_Qvalue(action, n_action) for n_action in self.Field.get_actions(action)])\n",
    "        r_s_a, finish_flg = self.Field.get_val(action)\n",
    "        q_value = Q_s_a + self.alpha * ( r_s_a +  self.gamma * mQ_s_a - Q_s_a)\n",
    "        self.set_Qvalue(state, action, q_value)\n",
    "        return finish_flg\n",
    "\n",
    "\n",
    "    def get_Qvalue(self, state, action):\n",
    "        state = (state[0],state[1])\n",
    "        action = (action[0],action[1])\n",
    "        try:\n",
    "            return self.Qvalue[state][action]\n",
    "        except KeyError:\n",
    "            return 0.0\n",
    "\n",
    "    def set_Qvalue(self, state, action, q_value):\n",
    "        state = (state[0],state[1])\n",
    "        action = (action[0],action[1])\n",
    "        self.Qvalue.setdefault(state,{})\n",
    "        self.Qvalue[state][action] = q_value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if self.epsilon < random.random():\n",
    "            return random.choice(self.Field.get_actions(state))\n",
    "        else:\n",
    "            return self.choose_action_greedy(state)\n",
    "\n",
    "    def choose_action_greedy(self, state):\n",
    "        best_actions = []\n",
    "        max_q_value = -100\n",
    "        for a in self.Field.get_actions(state):\n",
    "            q_value = self.get_Qvalue(state, a)\n",
    "            if q_value > max_q_value:\n",
    "                best_actions = [a,]\n",
    "                max_q_value = q_value\n",
    "            elif q_value == max_q_value:\n",
    "                best_actions.append(a)\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def dump_Qvalue(self):\n",
    "        print(\"##### Dump Qvalue #####\")\n",
    "        for i, s in enumerate(self.Qvalue.keys()):\n",
    "            for a in self.Qvalue[s].keys():\n",
    "                print(\"\\t\\tQ(s, a): Q(%s, %s): %s\" % (str(s), str(a), str(self.Qvalue[s][a])))\n",
    "            if i != len(self.Qvalue.keys())-1: \n",
    "                print('\\t------------------state   action   reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "QL_solver = QLearning_Solver(maze_field, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(learning_count):\n",
    "    QL_solver.qlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Dump Qvalue #####\n",
      "\t\tQ(s, a): Q((0, 1), (1, 1)): 6.576643030082795\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 1), (2, 1)): 8.418492255647552\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 1), (1, 1)): 6.576643030082795\n",
      "\t\tQ(s, a): Q((2, 1), (2, 2)): 10.464991395163949\n",
      "\t\tQ(s, a): Q((2, 1), (3, 1)): 10.464991395163949\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 2), (2, 1)): 8.418492255647552\n",
      "\t\tQ(s, a): Q((2, 2), (3, 2)): 12.738879327959948\n",
      "\t\tQ(s, a): Q((2, 2), (2, 3)): 11.738879327959948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 2), (3, 1)): 10.464991395163949\n",
      "\t\tQ(s, a): Q((3, 2), (4, 2)): 11.464991395163949\n",
      "\t\tQ(s, a): Q((3, 2), (2, 2)): 10.464991395163949\n",
      "\t\tQ(s, a): Q((3, 2), (3, 3)): 14.154310364399945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 1), (4, 1)): 9.31849225564755\n",
      "\t\tQ(s, a): Q((3, 1), (3, 2)): 12.738879327959948\n",
      "\t\tQ(s, a): Q((3, 1), (2, 1)): 8.418492255647552\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 1), (4, 2)): 11.464991395163949\n",
      "\t\tQ(s, a): Q((4, 1), (3, 1)): 10.464991395163949\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 2), (4, 1)): 9.31849225564755\n",
      "\t\tQ(s, a): Q((4, 2), (3, 2)): 12.738879327959948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 3), (3, 3)): 14.154310364399945\n",
      "\t\tQ(s, a): Q((2, 3), (1, 3)): 9.56499139516395\n",
      "\t\tQ(s, a): Q((2, 3), (2, 4)): 13.154310364399945\n",
      "\t\tQ(s, a): Q((2, 3), (2, 2)): 10.464991395163949\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 3), (3, 2)): 12.738879327959948\n",
      "\t\tQ(s, a): Q((3, 3), (3, 4)): 15.727011515999942\n",
      "\t\tQ(s, a): Q((3, 3), (2, 3)): 11.738879327959948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 3), (1, 4)): 10.838879327959948\n",
      "\t\tQ(s, a): Q((1, 3), (2, 3)): 11.738879327959948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 4), (2, 4)): 13.154310364399945\n",
      "\t\tQ(s, a): Q((1, 4), (1, 3)): 9.56499139516395\n",
      "\t\tQ(s, a): Q((1, 4), (1, 5)): 12.254310364399945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 4), (1, 4)): 10.838879327959948\n",
      "\t\tQ(s, a): Q((2, 4), (2, 5)): 14.727011515999942\n",
      "\t\tQ(s, a): Q((2, 4), (3, 4)): 15.727011515999942\n",
      "\t\tQ(s, a): Q((2, 4), (2, 3)): 11.738879327959948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 5), (1, 5)): 12.254310364399945\n",
      "\t\tQ(s, a): Q((2, 5), (2, 4)): 13.154310364399945\n",
      "\t\tQ(s, a): Q((2, 5), (2, 6)): 16.384457239999943\n",
      "\t\tQ(s, a): Q((2, 5), (3, 5)): 17.47445723999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 5), (1, 6)): 14.646011515999945\n",
      "\t\tQ(s, a): Q((1, 5), (2, 5)): 14.727011515999942\n",
      "\t\tQ(s, a): Q((1, 5), (1, 4)): 10.838879327959948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 6), (1, 7)): 17.384457239999943\n",
      "\t\tQ(s, a): Q((1, 6), (2, 6)): 16.384457239881172\n",
      "\t\tQ(s, a): Q((1, 6), (1, 5)): 12.254310364397538\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 7), (1, 6)): 14.646011515999945\n",
      "\t\tQ(s, a): Q((1, 7), (2, 7)): 19.316063599999943\n",
      "\t\tQ(s, a): Q((1, 7), (1, 8)): 14.646011515999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 6), (1, 6)): 14.646011515999945\n",
      "\t\tQ(s, a): Q((2, 6), (2, 5)): 14.727011515999942\n",
      "\t\tQ(s, a): Q((2, 6), (2, 7)): 19.316063599999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 4), (4, 4)): 16.47445723999994\n",
      "\t\tQ(s, a): Q((3, 4), (3, 5)): 17.47445723999994\n",
      "\t\tQ(s, a): Q((3, 4), (2, 4)): 13.154310364399945\n",
      "\t\tQ(s, a): Q((3, 4), (3, 3)): 14.154310364399945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 4), (5, 4)): 19.41606359999994\n",
      "\t\tQ(s, a): Q((4, 4), (4, 5)): 19.41606359999994\n",
      "\t\tQ(s, a): Q((4, 4), (3, 4)): 15.727011515999942\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 4), (5, 3)): 16.47445723999994\n",
      "\t\tQ(s, a): Q((5, 4), (4, 4)): 16.47445723999994\n",
      "\t\tQ(s, a): Q((5, 4), (5, 5)): 21.573403999999943\n",
      "\t\tQ(s, a): Q((5, 4), (6, 4)): 21.573403999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 3), (6, 3)): 19.41606359999994\n",
      "\t\tQ(s, a): Q((5, 3), (5, 4)): 19.41606359999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 3), (7, 3)): 21.573403999999943\n",
      "\t\tQ(s, a): Q((6, 3), (5, 3)): 16.474457239999925\n",
      "\t\tQ(s, a): Q((6, 3), (6, 4)): 21.573403999999943\n",
      "\t\tQ(s, a): Q((6, 3), (6, 2)): 17.474457239998046\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 3), (6, 3)): 19.41606359947087\n",
      "\t\tQ(s, a): Q((7, 3), (7, 4)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((7, 3), (7, 2)): 18.416063598304376\n",
      "\t\tQ(s, a): Q((7, 3), (8, 3)): 22.696459999998115\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 4), (7, 3)): 21.573403999999943\n",
      "\t\tQ(s, a): Q((7, 4), (6, 4)): 21.573403999999943\n",
      "\t\tQ(s, a): Q((7, 4), (8, 4)): 26.32939999999995\n",
      "\t\tQ(s, a): Q((7, 4), (7, 5)): 27.868399999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 4), (5, 4)): 19.41606359999994\n",
      "\t\tQ(s, a): Q((6, 4), (7, 4)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((6, 4), (6, 5)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((6, 4), (6, 3)): 19.41606359999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 5), (4, 4)): 16.47445723999994\n",
      "\t\tQ(s, a): Q((4, 5), (3, 5)): 17.47445723999994\n",
      "\t\tQ(s, a): Q((4, 5), (5, 5)): 21.573403999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 5), (3, 4)): 15.727011515999942\n",
      "\t\tQ(s, a): Q((3, 5), (2, 5)): 14.727011515999942\n",
      "\t\tQ(s, a): Q((3, 5), (4, 5)): 19.41606359999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 5), (6, 5)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((5, 5), (4, 5)): 19.41606359999994\n",
      "\t\tQ(s, a): Q((5, 5), (5, 6)): 24.081559999999943\n",
      "\t\tQ(s, a): Q((5, 5), (5, 4)): 19.41606359999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 5), (6, 6)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((6, 5), (7, 5)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((6, 5), (6, 4)): 21.573403999999943\n",
      "\t\tQ(s, a): Q((6, 5), (5, 5)): 21.573403999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 6), (6, 7)): 31.07599999999995\n",
      "\t\tQ(s, a): Q((6, 6), (6, 5)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((6, 6), (7, 6)): 32.075999999999944\n",
      "\t\tQ(s, a): Q((6, 6), (5, 6)): 24.081559999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 7), (7, 7)): 35.63999999999995\n",
      "\t\tQ(s, a): Q((6, 7), (6, 6)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((6, 7), (5, 7)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((6, 7), (6, 8)): 35.63999999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 7), (7, 8)): 39.59999999999996\n",
      "\t\tQ(s, a): Q((7, 7), (6, 7)): 31.07599999999995\n",
      "\t\tQ(s, a): Q((7, 7), (7, 6)): 32.075999999999944\n",
      "\t\tQ(s, a): Q((7, 7), (8, 7)): 38.59999999999996\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 8), (8, 8)): 43.99999999999997\n",
      "\t\tQ(s, a): Q((7, 8), (7, 7)): 35.63999999999995\n",
      "\t\tQ(s, a): Q((7, 8), (6, 8)): 35.63999999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 8), (9, 8)): 49.999999999999986\n",
      "\t\tQ(s, a): Q((8, 8), (8, 7)): 38.59999999999996\n",
      "\t\tQ(s, a): Q((8, 8), (7, 8)): 39.59999999999996\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 6), (5, 7)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((5, 6), (6, 6)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((5, 6), (5, 5)): 21.573403999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 7), (4, 7)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((5, 7), (6, 7)): 31.07599999999995\n",
      "\t\tQ(s, a): Q((5, 7), (5, 8)): 32.075999999999944\n",
      "\t\tQ(s, a): Q((5, 7), (5, 6)): 24.081559999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 7), (3, 7)): 22.573403999999943\n",
      "\t\tQ(s, a): Q((4, 7), (5, 7)): 27.868399999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 7), (4, 7)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((3, 7), (2, 7)): 19.316063599999943\n",
      "\t\tQ(s, a): Q((3, 7), (3, 8)): 20.316063599999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 5), (7, 6)): 32.075999999999944\n",
      "\t\tQ(s, a): Q((7, 5), (8, 5)): 30.36599999999995\n",
      "\t\tQ(s, a): Q((7, 5), (7, 4)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((7, 5), (6, 5)): 25.081559999999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 6), (7, 5)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((7, 6), (6, 6)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((7, 6), (7, 7)): 35.63999999999995\n",
      "\t\tQ(s, a): Q((7, 6), (8, 6)): 33.73999999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 5), (8, 4)): 26.32939999999995\n",
      "\t\tQ(s, a): Q((8, 5), (8, 6)): 33.73999999999995\n",
      "\t\tQ(s, a): Q((8, 5), (7, 5)): 27.868399999999944\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 4), (7, 4)): 25.081559999999943\n",
      "\t\tQ(s, a): Q((8, 4), (8, 3)): 22.69645999999995\n",
      "\t\tQ(s, a): Q((8, 4), (8, 5)): 30.36599999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 3), (7, 3)): 21.573403999977693\n",
      "\t\tQ(s, a): Q((8, 3), (8, 4)): 26.32939999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 2), (6, 2)): 17.47445676092675\n",
      "\t\tQ(s, a): Q((7, 2), (7, 3)): 21.573403999726835\n",
      "\t\tQ(s, a): Q((7, 2), (7, 1)): 15.574443329494683\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 2), (6, 1)): 15.726969799963307\n",
      "\t\tQ(s, a): Q((6, 2), (6, 3)): 19.41606359999994\n",
      "\t\tQ(s, a): Q((6, 2), (7, 2)): 18.416060185786115\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 1), (6, 2)): 17.47445522219822\n",
      "\t\tQ(s, a): Q((6, 1), (7, 1)): 15.573406851836937\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 7), (3, 7)): 22.573403999999943\n",
      "\t\tQ(s, a): Q((2, 7), (1, 7)): 17.384457239999943\n",
      "\t\tQ(s, a): Q((2, 7), (2, 6)): 16.384457239999943\n",
      "\t\tQ(s, a): Q((2, 7), (2, 8)): 17.28445723999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 8), (2, 8)): 17.28445723999991\n",
      "\t\tQ(s, a): Q((1, 8), (1, 7)): 17.384457239999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 8), (2, 7)): 19.31606359999981\n",
      "\t\tQ(s, a): Q((2, 8), (1, 8)): 14.646011515999929\n",
      "\t\tQ(s, a): Q((2, 8), (3, 8)): 20.316063599999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 8), (3, 7)): 22.573403999999943\n",
      "\t\tQ(s, a): Q((3, 8), (2, 8)): 17.28445723999994\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 8), (5, 7)): 27.868399999999944\n",
      "\t\tQ(s, a): Q((5, 8), (6, 8)): 35.63999999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 8), (5, 8)): 32.075999999999944\n",
      "\t\tQ(s, a): Q((6, 8), (7, 8)): 39.59999999999996\n",
      "\t\tQ(s, a): Q((6, 8), (6, 7)): 31.07599999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 6), (8, 5)): 30.36599999999995\n",
      "\t\tQ(s, a): Q((8, 6), (7, 6)): 32.075999999999944\n",
      "\t\tQ(s, a): Q((8, 6), (8, 7)): 38.59999999999996\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 7), (7, 7)): 35.63999999999995\n",
      "\t\tQ(s, a): Q((8, 7), (8, 6)): 33.73999999999995\n",
      "\t\tQ(s, a): Q((8, 7), (8, 8)): 43.99999999999997\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 1), (6, 1)): 15.726707582907414\n",
      "\t\tQ(s, a): Q((7, 1), (7, 2)): 18.416062768263746\n"
     ]
    }
   ],
   "source": [
    "QL_solver.dump_Qvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: [0, 1] -> action: [1, 1] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  @@   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 1 \t score: -1.0\n",
      "\n",
      "current state: [1, 1] -> action: [2, 1] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  @@  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 2 \t score: -2.0\n",
      "\n",
      "current state: [2, 1] -> action: [3, 1] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  @@   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 3 \t score: -3.0\n",
      "\n",
      "current state: [3, 1] -> action: [3, 2] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1  @@   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 4 \t score: -3.0\n",
      "\n",
      "current state: [3, 2] -> action: [3, 3] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  @@   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 5 \t score: -3.0\n",
      "\n",
      "current state: [3, 3] -> action: [3, 4] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0  @@   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 6 \t score: -3.0\n",
      "\n",
      "current state: [3, 4] -> action: [3, 5] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0  @@   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 7 \t score: -3.0\n",
      "\n",
      "current state: [3, 5] -> action: [4, 5] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1  @@   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 8 \t score: -3.0\n",
      "\n",
      "current state: [4, 5] -> action: [5, 5] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  @@  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 9 \t score: -4.0\n",
      "\n",
      "current state: [5, 5] -> action: [6, 5] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1  @@  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 10 \t score: -4.0\n",
      "\n",
      "current state: [6, 5] -> action: [6, 6] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  @@  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 11 \t score: -5.0\n",
      "\n",
      "current state: [6, 6] -> action: [7, 6] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1  @@   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 12 \t score: -5.0\n",
      "\n",
      "current state: [7, 6] -> action: [7, 7] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0  @@   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 13 \t score: -5.0\n",
      "\n",
      "current state: [7, 7] -> action: [7, 8] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0  @@   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 14 \t score: -5.0\n",
      "\n",
      "current state: [7, 8] -> action: [8, 8] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  @@   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 15 \t score: -6.0\n",
      "\n",
      "current state: [8, 8] -> action: [9, 8] \n",
      "\t  #   S   #   #   #   #   #   #   #   # \n",
      "\t  #  -1   #  -1  -1  -1  -1   0  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1  -1  -1   # \n",
      "\t  #  -1   0   0   0   0   #   0   0   # \n",
      "\t  #  -1   0   #  -1   0   #   0   #   # \n",
      "\t  #   #   #  -1   0  -1  -1  -1   0   # \n",
      "\t  #   0   0   0  -1   0  -1  -1   0   # \n",
      "\t  #  -1  -1  -1   0  -1   0   0   0   # \n",
      "\t  #   #   #  -1  -1   0  -1  -1  -1   # \n",
      "\t  #   #   #   #   #   #   #   #  @@   # \n",
      "current step: 16 \t score: 44.0\n",
      "\n",
      "Goal!\n"
     ]
    }
   ],
   "source": [
    "QL_solver.qlearn(greedy_flg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
